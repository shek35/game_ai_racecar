{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Racing Implementation of Twin Delayed Deep Deterministic Policy Gradients (TD3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an implementation based on the following source material:\n",
    "\n",
    "Paper: https://arxiv.org/abs/1802.09477\n",
    "\n",
    "Source: https://github.com/sfujim/TD3/blob/master/TD3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from car_racing import CarRacing\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange\n",
    "from pygame_screen_record.ScreenRecorder import ScreenRecorder, cleanup, add_codec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting to an available device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "add_codec(\"mp4\", \"mp4v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action, hidden_dims):\n",
    "        \"\"\"\n",
    "        Initializes the Actor class.\n",
    "\n",
    "        Args:\n",
    "            state_dim (int): Dimension of the input state.\n",
    "            action_dim (int): Dimension of the output action.\n",
    "            max_action (float): Maximum value of the action.\n",
    "            hidden_dims (list): List of integers representing the number of neurons in each hidden layer.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_layer_neurons = state_dim\n",
    "        for neurons in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_layer_neurons, neurons))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_layer_neurons = neurons\n",
    "\n",
    "        layers.append(nn.Linear(prev_layer_neurons, action_dim))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass of the neural network.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): The input state tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after applying the forward pass.\n",
    "        \"\"\"\n",
    "        return self.max_action * torch.tanh(self.layers(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dims):\n",
    "        \"\"\"\n",
    "        Initialize the Critic class.\n",
    "\n",
    "        Args:\n",
    "            state_dim (int): Dimension of the state space.\n",
    "            action_dim (int): Dimension of the action space.\n",
    "            hidden_dims (list): List of integers representing the number of neurons in each hidden layer.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # Q1 architecture\n",
    "        layers_q1 = []\n",
    "        prev_layer_neurons = state_dim + action_dim\n",
    "        for neurons in hidden_dims:\n",
    "            layers_q1.append(nn.Linear(prev_layer_neurons, neurons))\n",
    "            layers_q1.append(nn.ReLU())\n",
    "            prev_layer_neurons = neurons\n",
    "\n",
    "        layers_q1.append(nn.Linear(prev_layer_neurons, 1))\n",
    "        self.layers_q1 = nn.Sequential(*layers_q1)\n",
    "\n",
    "        # Q2 architecture\n",
    "        layers_q2 = []\n",
    "        prev_layer_neurons = state_dim + action_dim\n",
    "        for neurons in hidden_dims:\n",
    "            layers_q2.append(nn.Linear(prev_layer_neurons, neurons))\n",
    "            layers_q2.append(nn.ReLU())\n",
    "            prev_layer_neurons = neurons\n",
    "\n",
    "        layers_q2.append(nn.Linear(prev_layer_neurons, 1))\n",
    "        self.layers_q2 = nn.Sequential(*layers_q2)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Args:\n",
    "            state (torch.Tensor): The input state tensor.\n",
    "            action (torch.Tensor): The input action tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The Q-values for the given state-action pair.\n",
    "        \"\"\"\n",
    "        sa = torch.cat([state, action], 1)\n",
    "        q1 = self.layers_q1(sa)\n",
    "        q2 = self.layers_q2(sa)\n",
    "        return q1, q2\n",
    "\n",
    "    def Q1(self, state, action):\n",
    "        \"\"\"\n",
    "        Compute the Q-value for a given state-action pair.\n",
    "\n",
    "        Parameters:\n",
    "        state (torch.Tensor): The state tensor.\n",
    "        action (torch.Tensor): The action tensor.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: The Q-value tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        sa = torch.cat([state, action], 1)\n",
    "        return self.layers_q1(sa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD3 Implemenatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        action_dim,\n",
    "        hidden_dims,\n",
    "        max_action,\n",
    "        discount=0.99,\n",
    "        tau=0.005,\n",
    "        policy_noise=0.2,\n",
    "        noise_clip=0.5,\n",
    "        policy_freq=2,\n",
    "        learning_rate=3e-4\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the TD3 (Twin Delayed Deep Deterministic Policy Gradient) agent.\n",
    "\n",
    "        Args:\n",
    "            state_dim (int): Dimension of the state space.\n",
    "            action_dim (int): Dimension of the action space.\n",
    "            hidden_dims (list): List of integers representing the sizes of hidden layers in the actor and critic networks.\n",
    "            max_action (float): Max upper bound for action values.\n",
    "            discount (float, optional): Discount factor for future rewards. Defaults to 0.99.\n",
    "            tau (float, optional): Target network update rate. Defaults to 0.005.\n",
    "            policy_noise (float, optional): Noise added to target policy during critic update. Defaults to 0.2.\n",
    "            noise_clip (float, optional): Range to clip target policy noise. Defaults to 0.5.\n",
    "            policy_freq (int, optional): Frequency of delayed policy updates. Defaults to 2.\n",
    "            learning_rate (float, optional): Learning rate for the actor and critic networks. Defaults to 3e-4.\n",
    "        \"\"\"\n",
    "        self.actor = Actor(state_dim, action_dim, max_action, hidden_dims).to(device)\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim, hidden_dims).to(device)\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.discount = discount\n",
    "        self.tau = tau\n",
    "        self.policy_noise = policy_noise\n",
    "        self.noise_clip = noise_clip\n",
    "        self.policy_freq = policy_freq\n",
    "\n",
    "        self.total_it = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Selects an action based on the given state.\n",
    "\n",
    "        Parameters:\n",
    "        state (numpy.ndarray): The current state of the environment.\n",
    "\n",
    "        Returns:\n",
    "        numpy.ndarray: The selected action.\n",
    "\n",
    "        \"\"\"\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "\n",
    "    def train(self, replay_memory, batch_size=256):\n",
    "        \"\"\"\n",
    "        Trains the TD3 agent using the given replay buffer.\n",
    "\n",
    "        Args:\n",
    "            replay_memory (ReplayMemory): The replay buffer containing the agent's experience.\n",
    "            batch_size (int, optional): The batch size for sampling from the replay buffer. Defaults to 256.\n",
    "        \"\"\"\n",
    "        self.total_it += 1\n",
    "\n",
    "        # Sample replay memory\n",
    "        state, action, next_state, reward, not_done = replay_memory.sample(batch_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            #clip noise\n",
    "            noise = (\n",
    "                    torch.randn_like(action) * self.policy_noise\n",
    "            ).clamp(-self.noise_clip, self.noise_clip)\n",
    "\n",
    "            # action selection\n",
    "            next_action = (\n",
    "                    self.actor_target(next_state) + noise\n",
    "            ).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            # Compute target Q value\n",
    "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + not_done * self.discount * target_Q\n",
    "\n",
    "        # Q estimates\n",
    "        current_Q1, current_Q2 = self.critic(state, action)\n",
    "\n",
    "        # Compute critic loss\n",
    "        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "        # Optimize critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Delayed policy updates\n",
    "        if self.total_it % self.policy_freq == 0:\n",
    "\n",
    "            # Compute actor loss\n",
    "            actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
    "\n",
    "            # Optimize the actor\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Update frozen target models\n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "\n",
    "    def save(self, filename):\n",
    "        \"\"\"\n",
    "        Save the state of the TD3 agent to the specified file.\n",
    "\n",
    "        Parameters:\n",
    "        - filename (str): The name of the file to save the agent's state.\n",
    "\n",
    "        Returns:\n",
    "        - None\n",
    "        \"\"\"\n",
    "        torch.save(self.critic.state_dict(), filename + \"_critic\")\n",
    "        torch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer\")\n",
    "\n",
    "        torch.save(self.actor.state_dict(), filename + \"_actor\")\n",
    "        torch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer\")\n",
    "\n",
    "    def load(self, filename):\n",
    "        \"\"\"\n",
    "        Loads the saved model parameters from the specified file.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The name of the file (without extension) containing the saved model parameters.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.critic.load_state_dict(torch.load(filename + \"_critic\"))\n",
    "        self.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer\"))\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "\n",
    "        self.actor.load_state_dict(torch.load(filename + \"_actor\"))\n",
    "        self.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer\"))\n",
    "        self.actor_target = copy.deepcopy(self.actor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Memory is data-structure where we store previous experiences so that we can re-sample and train on them. These experiences start with a random policy and then picked according to the current policy from TD3. Note we store whether the state is not done rather than done for use in target Q calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "    def __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
    "        \"\"\"Replay memory implemented as a circular buffer.\n",
    "\n",
    "        Experiences will be removed in a FIFO manner after reaching maximum\n",
    "        buffer size.\n",
    "\n",
    "        Args:\n",
    "            - max_size: Maximum size of the buffer.\n",
    "            - state_size: Size of the state-space features for the environment.\n",
    "            - action_dim: Size of the action space for the environment.\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "\n",
    "        # preallocating all the required memory, for speed concerns\n",
    "        self.state = np.zeros((max_size, state_dim))\n",
    "        self.action = np.zeros((max_size, action_dim))\n",
    "        self.next_state = np.zeros((max_size, state_dim))\n",
    "        self.reward = np.zeros((max_size, 1))\n",
    "        self.not_done = np.zeros((max_size, 1))\n",
    "\n",
    "        # pointer to the current location in the circular buffer\n",
    "        self.idx = 0\n",
    "        # indicates number of transitions currently stored in the buffer\n",
    "        self.size = 0\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        \"\"\"Add a transition to the buffer.\n",
    "\n",
    "        :param state:  state_dim np.ndarray of state-features.\n",
    "        :param action: action_dim size action.\n",
    "        :param reward:  float reward.\n",
    "        :param next_state:  state_dim np.ndarray of state-features.\n",
    "        :param done:  boolean value indicating the end of an episode.\n",
    "        \"\"\"\n",
    "\n",
    "        # Store the input values into the appropriate attributes, using the current buffer position `self.idx`\n",
    "        self.state[self.idx] = state\n",
    "        self.action[self.idx] = action\n",
    "        self.next_state[self.idx] = next_state\n",
    "        self.reward[self.idx] = reward\n",
    "        self.not_done[self.idx] = 1. - done\n",
    "\n",
    "        # circulate the pointer to the next position\n",
    "        self.idx = (self.idx + 1) % self.max_size\n",
    "        # update the current buffer size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "\n",
    "        If the buffer contains less that `batch_size` transitions, sample all\n",
    "        of them.\n",
    "\n",
    "        :param batch_size:  Number of transitions to sample.\n",
    "        \"\"\"\n",
    "        # sample_indices = np.random.randint(0, self.size, size=batch_size)\n",
    "        # Randomly sample an appropriate number of transitions without replacement\n",
    "        # If the buffer contains less than `batch_size` transitions, return all of them\n",
    "        if self.size < batch_size:\n",
    "            ind = np.random.choice(self.size, self.size, replace=False)\n",
    "        else:\n",
    "            ind = np.random.choice(self.size, batch_size, replace=False)\n",
    "\n",
    "        return (\n",
    "            torch.FloatTensor(self.state[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.action[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.next_state[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.reward[ind]).to(self.device),\n",
    "            torch.FloatTensor(self.not_done[ind]).to(self.device)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Load td3 car implementation. If you wish to have a different seed for each reset, set the value of track seed to None. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td3_car(save_file_name='default_model', \n",
    "                max_train_timesteps=2_000_000,\n",
    "                eval_interval=5000, \n",
    "                load_file='', \n",
    "                track_seed=123, \n",
    "                hidden_dims=[512, 256],\n",
    "                max_action=1,\n",
    "                action_dim=3, \n",
    "                state_size=15, \n",
    "                random_policy_steps=5000,\n",
    "                batch_size=512,\n",
    "                explore_noise=0.1,\n",
    "                policy_noise=0.2,\n",
    "                nose_clip=0.5,\n",
    "                deterministic_train=True,\n",
    "                deterministic_eval=True):\n",
    "    \"\"\"\n",
    "    Train and evaluate a TD3 (Twin Delayed Deep Deterministic Policy Gradient) agent for a car racing environment.\n",
    "\n",
    "    Parameters:\n",
    "    - save_file_name (str): The name of the file to save the trained model.\n",
    "    - max_train_timesteps (int): The maximum number of training timesteps.\n",
    "    - eval_only (bool): If True, only perform evaluation without training.\n",
    "    - eval_interval (int): The interval (in timesteps) between evaluations.\n",
    "    - load_file (str): The name of the file to load a pre-trained model from.\n",
    "    - track_seed (int): The seed for the car racing environment.\n",
    "    - hidden_dims (list): The dimensions of the hidden layers in the neural network.\n",
    "    - max_action (float): The maximum action value.\n",
    "    - action_dim (int): The dimension of the action space.\n",
    "    - state_size (int): The dimension of the state space.\n",
    "    - random_policy_steps (int): The number of timesteps to follow a random policy before using the learned policy.\n",
    "    - batch_size (int): The size of the training batch.\n",
    "    - explore_noise (float): The amount of noise added to the action during exploration.\n",
    "    - policy_noise (float): The amount of noise added to the target policy during training.\n",
    "    - nose_clip (float): The clipping value for the noise added to the target policy during training.\n",
    "    - deterministic_train (bool): If True, use a deterministic policy during training.\n",
    "    - deterministic_eval (bool): If True, use a deterministic policy during evaluation.\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    train_env = CarRacing(render_mode=\"none\", continuous=True, deterministic=deterministic_train)\n",
    "    eval_env = CarRacing(render_mode=\"human\", continuous=True, deterministic=deterministic_eval)\n",
    "    policy = TD3(state_dim=state_size, action_dim=action_dim,\n",
    "                 hidden_dims=hidden_dims,\n",
    "                 max_action=max_action, policy_noise=policy_noise, noise_clip=nose_clip)\n",
    "\n",
    "    if load_file != '':\n",
    "        policy.load(f\"./{load_file}\")\n",
    "\n",
    "    print(f'track_seed: {track_seed}')\n",
    "    state = train_env.reset(seed=track_seed)\n",
    "    train_rewards = []\n",
    "    eval_rewards = []\n",
    "    max_eval_reward = 0\n",
    "    max_train_reward = 0\n",
    "    episode_reward = 0\n",
    "    episode_timesteps = 0\n",
    "    episode_num = 0\n",
    "\n",
    "    replay_memory = ReplayMemory(state_size, action_dim)\n",
    "\n",
    "    eval_reward = eval_policy(policy, eval_env, track_seed)\n",
    "    eval_rewards.append(eval_reward)\n",
    "\n",
    "    for t in trange(1, max_train_timesteps + 1, desc=\"Training\"):\n",
    "        episode_timesteps += 1\n",
    "\n",
    "        # Select action randomly or according to policy\n",
    "        if t < random_policy_steps:\n",
    "            action = train_env.action_space.sample()\n",
    "        else:\n",
    "            action = (\n",
    "                    policy.select_action(np.array(state))\n",
    "                    + np.random.normal(0, max_action * explore_noise, size=action_dim)\n",
    "            ).clip(-max_action, max_action)\n",
    "\n",
    "        # Perform action\n",
    "        next_state, reward, terminated, truncated = train_env.step(action)\n",
    "        done = float(terminated or truncated)\n",
    "\n",
    "        # Store data in replay buffer\n",
    "        replay_memory.add(state, action, next_state, reward, done)\n",
    "\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Train agent after collecting sufficient data\n",
    "        if t >= random_policy_steps:\n",
    "            policy.train(replay_memory, batch_size)\n",
    "\n",
    "        if done:\n",
    "            print(\n",
    "                f\"Total steps: {t + 1}, \"\n",
    "                f\"Episode Num: {episode_num + 1}, \"\n",
    "                f\"Episode steps: {episode_timesteps}, \"\n",
    "                f\"Reward: {episode_reward:.3f}\")\n",
    "            state, done = train_env.reset(), 0\n",
    "            if episode_reward > max_train_reward:\n",
    "                max_train_reward = episode_reward\n",
    "            # append to both train and eval to keep them with the same number\n",
    "            train_rewards.append(episode_reward)\n",
    "            eval_rewards.append(eval_reward)\n",
    "            # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
    "            episode_reward = 0\n",
    "            episode_timesteps = 0\n",
    "            episode_num += 1\n",
    "            plot_durations(test_rewards=eval_rewards, train_rewards=train_rewards, hidden_dims=hidden_dims, eval_interval=eval_interval, batch_size=batch_size,\n",
    "                           timestep=t, max_training_reward=max_train_reward)\n",
    "\n",
    "        if t % eval_interval == 0:\n",
    "            # append to both train and eval to keep them with the same number\n",
    "            eval_reward = eval_policy(policy, eval_env, track_seed)\n",
    "            train_rewards.append(episode_reward)\n",
    "            eval_rewards.append(eval_reward)\n",
    "\n",
    "            if eval_reward > max_eval_reward:\n",
    "                max_eval_reward = eval_reward\n",
    "                if max_eval_reward > 900:\n",
    "                    policy.save(f\"./models/{save_file_name}\")\n",
    "                    print(f\"saved model {save_file_name}\")\n",
    "\n",
    "            plot_durations(test_rewards=eval_rewards, train_rewards=train_rewards, batch_size=batch_size,\n",
    "                           timestep=t, max_training_reward=max_train_reward, hidden_dims=hidden_dims, eval_interval=eval_interval)\n",
    "\n",
    "    plot_durations(test_rewards=eval_rewards, train_rewards=train_rewards, hidden_dims=hidden_dims, eval_interval=eval_interval, show_result=True,\n",
    "                   timestep=max_train_timesteps+1, max_training_reward=max_train_reward, batch_size=batch_size)\n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "    train_env.close()\n",
    "    eval_env.close()\n",
    "    print(f'total time: {time.time() - start}')\n",
    "\n",
    "\n",
    "def eval_policy(policy, env, track_seed):\n",
    "    \"\"\"\n",
    "    Evaluates a given policy on the environment.\n",
    "\n",
    "    Args:\n",
    "        policy: The policy to evaluate.\n",
    "        env: The environment to evaluate the policy on.\n",
    "        track_seed: The seed value for the environment.\n",
    "\n",
    "    Returns:\n",
    "        total_reward: The total reward obtained by the policy during evaluation.\n",
    "    \"\"\"\n",
    "    done = False\n",
    "    state = env.reset(seed=track_seed)\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = policy.select_action(np.array(state))\n",
    "        state, reward, done, truncated = env.step(action)\n",
    "        total_reward += reward\n",
    "        done = done or truncated\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "def plot_durations(test_rewards, \n",
    "                   train_rewards, \n",
    "                   timestep, \n",
    "                   max_training_reward, \n",
    "                   eval_interval,\n",
    "                   show_result=False,\n",
    "                   ):\n",
    "    \"\"\"\n",
    "    Plot the rewards over episodes for the TD3 implementation.\n",
    "\n",
    "    Args:\n",
    "        test_rewards (list): List of rewards obtained during testing.\n",
    "        train_rewards (list): List of rewards obtained during training.\n",
    "        timestep (int): Current timestep.\n",
    "        max_training_reward (float): Maximum training reward.\n",
    "        eval_interval (int): Interval at which evaluation is performed.\n",
    "        show_result (bool, optional): Whether to show the result or not. Defaults to False.\n",
    "    \"\"\"\n",
    "    fig = plt.figure(1, figsize=(9, 6))\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title(\"Reward over episodes for TD3 Implementation\")\n",
    "\n",
    "    next_eval = abs((timestep % eval_interval) - eval_interval)\n",
    "    plt.xlabel(f'Episode ({len(train_rewards)}), '\n",
    "               f'next eval: {next_eval}', fontsize=20)\n",
    "    plt.ylabel(f'Reward (max eval: {max(test_rewards):5.1f}'\n",
    "               f', max train: {max_training_reward:5.1f})', fontsize=15)\n",
    "    plt.plot(train_rewards, label='Train Reward')\n",
    "    plt.plot(test_rewards, label='Test Reward')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    fig.canvas.start_event_loop(0.001)  # this updates the plot and doesn't steal window focus\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "def load_td3_car(action_dim=3, \n",
    "                 state_size=15, \n",
    "                 max_action=1, \n",
    "                 load_file=\"default_model\", \n",
    "                 num_runs=1, \n",
    "                 deterministic_eval=True, \n",
    "                 track_seed=123, \n",
    "                 hidden_dims=[512, 256], \n",
    "                 policy_noise=0.2, \n",
    "                 nose_clip=0.5,\n",
    "                 record=False,\n",
    "                 recording_name=\"recordings/default_recording\"):\n",
    "    \"\"\"\n",
    "    Loads a TD3 car model and evaluates its performance on a given environment.\n",
    "\n",
    "    Parameters:\n",
    "    - action_dim (int): Dimension of the action space.\n",
    "    - state_size (int): Dimension of the state space.\n",
    "    - max_action (float): Maximum value of the action.\n",
    "    - load_file (str): File path to load the pre-trained model from.\n",
    "    - num_runs (int): Number of evaluation runs to perform.\n",
    "    - deterministic_eval (bool): Whether to perform deterministic evaluation.\n",
    "    - track_seed (int): Seed value for the evaluation environment.\n",
    "    - hidden_dims (list): List of hidden layer dimensions for the policy network.\n",
    "    - policy_noise (float): Standard deviation of the policy noise.\n",
    "    - nose_clip (float): Clipping value for the policy noise.\n",
    "    - record (bool): Whether to record the evaluation runs.\n",
    "    - recording_name (str): File path to save the recordings.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    eval_env = CarRacing(render_mode=\"human\", continuous=True, deterministic=deterministic_eval)\n",
    "    policy = TD3(state_dim=state_size, action_dim=action_dim,\n",
    "                 hidden_dims=hidden_dims,\n",
    "                 max_action=max_action, policy_noise=policy_noise, noise_clip=nose_clip)\n",
    "\n",
    "    policy.load(f\"./{load_file}\")\n",
    "    times = []\n",
    "    eval_rewards = []\n",
    "    for run in range(num_runs):\n",
    "        if record:\n",
    "            recorder = ScreenRecorder(60) # Pass your desired fps\n",
    "            recorder.start_rec() # Start recording\n",
    "        print(f'track_seed: {track_seed}')\n",
    "        start = time.time()\n",
    "        eval_reward = eval_policy(policy, eval_env, track_seed)\n",
    "        total_time = time.time() - start\n",
    "        if record:\n",
    "            recorder.stop_rec().get_single_recording().save((recording_name + \"_\" + str(run), \"mp4\"))\n",
    "        print(f'total reward: {eval_reward}')\n",
    "        print(f'total time: {total_time}')\n",
    "        times.append(total_time)\n",
    "        eval_rewards.append(eval_reward)\n",
    "    print(f'average time: {np.average(times)}')\n",
    "    print(f'average reward: {np.average(eval_rewards)}')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td3_car(\"test_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "track_seed: 123\n",
      "total reward: 911.2888198757612\n",
      "total time: 17.03707194328308\n",
      "track_seed: 123\n",
      "total reward: 911.2888198757612\n",
      "total time: 16.760058879852295\n",
      "track_seed: 123\n",
      "total reward: 911.2888198757612\n",
      "total time: 17.62105417251587\n",
      "average time: 17.139394998550415\n",
      "average reward: 911.2888198757613\n"
     ]
    }
   ],
   "source": [
    "load_td3_car(load_file=\"models/test_model\", num_runs=3, record=False, recording_name=\"recordings/test_recording\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
